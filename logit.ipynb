{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5745103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Coefficients: [ 2.  -1.5]\n",
      "Estimated Coefficients: [ 2.01128805 -1.53752281]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.80      0.80      0.80      1222\n",
      "     Class 1       0.81      0.80      0.81      1278\n",
      "\n",
      "    accuracy                           0.80      2500\n",
      "   macro avg       0.80      0.80      0.80      2500\n",
      "weighted avg       0.80      0.80      0.80      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(123)  # Set random seed for reproducibility\n",
    "\n",
    "# Generate independent variables (features)\n",
    "n_samples = 10000\n",
    "X = np.random.randn(n_samples, 2)  # Two features\n",
    "\n",
    "true_coefficients = np.array([2.0, -1.5])\n",
    "\n",
    "# Compute logits and probabilities\n",
    "logits = X @ true_coefficients  # Linear combination of features and coefficients\n",
    "probs = 1 / (1 + np.exp(-logits))  # Apply logistic function\n",
    "\n",
    "# Generate binary outcomes (labels)\n",
    "y = np.random.binomial(1, probs)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "model = LogisticRegression(fit_intercept=False)  # No intercept for simplicity\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "estimated_coefficients = model.coef_[0]\n",
    "print(\"True Coefficients:\", true_coefficients)\n",
    "print(\"Estimated Coefficients:\", estimated_coefficients)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "report = classification_report(y_test, y_pred, target_names=[\"Class 0\", \"Class 1\"])\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7943737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit  # Sigmoid (inverse logit) function\n",
    "from scipy.optimize import root_scalar\n",
    "\n",
    "# Set parameters\n",
    "n_samples = 10000         # Total number of samples\n",
    "n_features = 5          # Total number of features\n",
    "desired_ratio = 0.1     # Desired spam ratio (5% positives)\n",
    "#np.random.seed(42)\n",
    "\n",
    "# Step 1: Generate the feature matrix X from a standard normal distribution.\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Step 2: Generate beta coefficients from a normal distribution.\n",
    "beta = np.random.randn(n_features)\n",
    "\n",
    "# Step 3: Calculate the linear predictor: z = X * beta.\n",
    "z = np.dot(X, beta)\n",
    "\n",
    "# Step 4: Calibrate an intercept so that the average probability is about desired_ratio.\n",
    "def calibrate_intercept(c, z, desired):\n",
    "    \"\"\"\n",
    "    Given z and a candidate intercept c,\n",
    "    returns the difference between the average probability and the desired ratio.\n",
    "    \"\"\"\n",
    "    p = expit(z + c)\n",
    "    return np.mean(p) - desired\n",
    "\n",
    "# Use a root-finding method (bisection) to find c so that mean(expit(z+c)) = desired_ratio.\n",
    "# We choose a bracket that we think will contain the root.\n",
    "sol = root_scalar(lambda c: calibrate_intercept(c, z, desired_ratio), bracket=[-20, 0], method='bisect')\n",
    "c = sol.root\n",
    "\n",
    "# Adjust the linear predictor.\n",
    "z_adjusted = z + c\n",
    "p = expit(z_adjusted)\n",
    "\n",
    "# Print the calibrated intercept and mean probability.\n",
    "print(\"Calibrated intercept:\", c)\n",
    "print(\"Mean probability after calibration:\", np.mean(p))\n",
    "\n",
    "# Step 5: Draw binary target values from a Bernoulli distribution using the calibrated probabilities.\n",
    "y = np.random.binomial(n=1, p=p, size=n_samples)\n",
    "\n",
    "# Check the achieved spam ratio.\n",
    "achieved_ratio = np.mean(y)\n",
    "print(\"Achieved positive ratio from Bernoulli sampling:\", achieved_ratio)\n",
    "\n",
    "# Optional: Create DataFrames for easier viewing.\n",
    "feature_columns = [f\"X{i}\" for i in range(1, n_features + 1)]\n",
    "X = pd.DataFrame(X, columns=feature_columns)\n",
    "y = pd.Series(y, name='target')\n",
    "\n",
    "print(\"\\nFirst 5 rows of features:\")\n",
    "print(X.head())\n",
    "print(\"\\nFirst 10 target values:\")\n",
    "print(y.head(10))\n",
    "y.value_counts(normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
